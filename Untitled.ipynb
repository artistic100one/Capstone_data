{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7401ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15595 15595\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Komoran tokenized La La Land review\n",
    "with open('./lalaland_komoran.txt', encoding='utf-8') as f:\n",
    "    sents = [sent.strip() for sent in f]\n",
    "\n",
    "with open('./lalaland.txt', encoding='utf-8') as f:\n",
    "    texts = [sent.strip() for sent in f]\n",
    "\n",
    "print(len(sents), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c7e4fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KeywordSummarizer' from 'textrankr' (C:\\Users\\User\\anaconda3\\lib\\site-packages\\textrankr\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2f9bc78a8b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextrankr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeywordSummarizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mkomoran_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'/NN'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'/XR'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'/VA'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'/VV'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KeywordSummarizer' from 'textrankr' (C:\\Users\\User\\anaconda3\\lib\\site-packages\\textrankr\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from textrankr import KeywordSummarizer\n",
    "\n",
    "def komoran_tokenize(sent):\n",
    "    words = sent.split()\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words\n",
    "\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = komoran_tokenize,\n",
    "    window = -1,\n",
    "    verbose = False\n",
    ")\n",
    "keywords = keyword_extractor.summarize(sents, topk=30)\n",
    "for word, rank in keywords:\n",
    "    print('{} ({:.3})'.format(word, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0a2605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈보라', '머리카락']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.nouns(\"눈보라에 차갑게 얼린 머리카락 먹고싶다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836289c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.nouns(\"미쿠 미쿠 하게 해줄게\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "bow1 = Counter(twitter.nouns(\"미쿠 미쿠 하게 해줄게\"))\n",
    "# Counter({'미쿠': 2})\n",
    "bow2 = Counter(twitter.nouns(\"미쿠 머리카락 맛있겠다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c3398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_index = sum((bow1 & bow2).values()) / sum((bow1 | bow2).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4915580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "\n",
    "    @staticmethod\n",
    "    def co_occurence(sentence1, sentence2):\n",
    "        p = sum((sentence1.bow & sentence2.bow).values())\n",
    "        q = sum((sentence1.bow | sentence2.bow).values())\n",
    "        return p / q if q else 0\n",
    "\n",
    "    def __init__(self, text, index=0):\n",
    "        self.index = index\n",
    "        self.text = text\n",
    "        self.nouns = twitter.nouns(self.text)\n",
    "        self.bow = Counter(self.nouns)\n",
    "\n",
    "    def __eq__(self, another):\n",
    "        return hasattr(another, 'index') and self.index == another.index\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37af562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    candidates = xplit('. ', '? ', '! ', '\\n', '.\\n')(text.strip())\n",
    "    sentences = []\n",
    "    index = 0\n",
    "    for candidate in candidates:\n",
    "        candidate = candidate.strip()\n",
    "        if len(candidate):\n",
    "            sentences.append(Sentence(candidate, index))\n",
    "            index += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c59e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(sentences):\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(sentences)\n",
    "    pairs = list(itertools.combinations(sentences, 2))\n",
    "    for eins, zwei in pairs:\n",
    "        graph.add_edge(eins, zwei, weight=Sentence.co_occurence(eins, zwei))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb42974a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b33702b8fdfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpagerank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = get_sentences(text)\n",
    "graph = build_graph(sentences)\n",
    "pagerank = networkx.pagerank(graph, weight='weight')\n",
    "reordered = sorted(pagerank, key=pagerank.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2b862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
